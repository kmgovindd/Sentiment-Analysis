{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e54bfdc2",
   "metadata": {
    "id": "e54bfdc2"
   },
   "source": [
    "# Text Analytics - Amazon Review Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4d210d",
   "metadata": {
    "id": "9a4d210d"
   },
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2686f3f3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2686f3f3",
    "outputId": "fd8ad419-d760-4924-f5c3-38938b405f12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: vaderSentiment in c:\\users\\work\\appdata\\roaming\\python\\python311\\site-packages (3.3.2)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from vaderSentiment) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (2024.2.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The syntax of the command is incorrect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: imblearn in c:\\users\\work\\appdata\\roaming\\python\\python311\\site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in c:\\programdata\\anaconda3\\lib\\site-packages (from imblearn) (0.11.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.11.4)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.2.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (2.2.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\users\\work\\appdata\\roaming\\python\\python311\\site-packages (4.39.2)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\work\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\work\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\work\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: contractions in c:\\users\\work\\appdata\\roaming\\python\\python311\\site-packages (0.1.73)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in c:\\users\\work\\appdata\\roaming\\python\\python311\\site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: anyascii in c:\\users\\work\\appdata\\roaming\\python\\python311\\site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
      "Requirement already satisfied: pyahocorasick in c:\\users\\work\\appdata\\roaming\\python\\python311\\site-packages (from textsearch>=0.0.21->contractions) (2.1.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: wordcloud in c:\\users\\work\\appdata\\roaming\\python\\python311\\site-packages (1.9.3)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from wordcloud) (1.26.4)\n",
      "Requirement already satisfied: pillow in c:\\programdata\\anaconda3\\lib\\site-packages (from wordcloud) (10.2.0)\n",
      "Requirement already satisfied: matplotlib in c:\\programdata\\anaconda3\\lib\\site-packages (from wordcloud) (3.8.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (23.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: textblob in c:\\users\\work\\appdata\\roaming\\python\\python311\\site-packages (0.18.0.post0)\n",
      "Requirement already satisfied: nltk>=3.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click->nltk>=3.8->textblob) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Work\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Work\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Work\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "!pip install vaderSentiment\n",
    "!pip install nltk\n",
    "# !pip install --upgrade scikit-learn\n",
    "#!pip uninstall scikit-learn --yes\n",
    "#!pip uninstall imblearn --yes\n",
    "!pip install scikit-learn==1.2.2|\n",
    "!pip install imblearn\n",
    "!pip install transformers\n",
    "!pip install contractions\n",
    "!pip install wordcloud\n",
    "!pip install textblob\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from wordcloud import WordCloud\n",
    "from textblob import TextBlob\n",
    "import logging\n",
    "%matplotlib inline\n",
    "from plotly import graph_objs as go\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "from collections import Counter\n",
    "import time\n",
    "# Data preprocessing\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "#from langdetect import detect, LangDetectException\n",
    "import contractions\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "import string\n",
    "from sklearn.base import TransformerMixin\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#import spacy\n",
    "from collections import Counter\n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    accuracy_score\n",
    ")\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from plotly import graph_objs as go\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "#from sklearn.metrics import plot_confusion_matrix\n",
    "#import xgboost as xgb\n",
    "#from xgboost import XGBClassifier\n",
    "from gensim.models import Word2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from scipy.special import softmax\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92ea63a",
   "metadata": {
    "id": "c92ea63a"
   },
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786b72c6",
   "metadata": {
    "id": "786b72c6"
   },
   "source": [
    "Overall topic modeling will provide us the insights into the main themes and sentiments expressed in high and low rated reviews\n",
    "- High-rated reviews tend to focus on positive experiences, product features, and usability\n",
    "- While low-rated reviews highlight dissatisfaction with various aspects of the product such as quality size and usability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa1740e2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aa1740e2",
    "outputId": "c668d86d-5842-4a8d-ff63-f95d73fd2af7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics for high ratings:\n",
      "Topic 1:  regulated residues affairs freaking dreammaker remnants jackets frames releases indentions\n",
      "Topic 2:  request feel diffuser hiccup balances project bother buggers dreammaker occassionally\n",
      "Topic 3:  established series aurilif fuzziness infrequently satisfactorily seas depend anarchy desire\n",
      "Topic 4:  instamorph schmetz reveal microsoft hanger dreammaker peace established pride restart\n",
      "Topic 5:  anarchy condusive desire marbled depend compress shinny primitive schmetz fuzziness\n",
      "Topic 6:  elle effortlessly b01avvuhqo deposit boo profession seas cardboard eating peels\n",
      "Topic 7:  register sign co eacute only hints daughters includes diamonds schmetz\n",
      "Topic 8:  length depression dreammaker handsome indentions responding handwork established pans remnants\n",
      "Topic 9:  endure established matel shinny marginally michels regret schmetz direction affairs\n",
      "Topic 10:  handsome freaking shrunk applicators remnants handstands reinforce established canning facebook\n",
      "\n",
      "Topics for low ratings:\n",
      "Topic 1:  the not and of it to money this are waste\n",
      "Topic 2:  horrible sprayed comment released button cast please grease additional ship\n",
      "Topic 3:  didnt explore tarnish sterling fake yards pfaff cricut useless skein\n",
      "Topic 4:  the it to and not this of is was for\n",
      "Topic 5:  damaged junk arrived include overlay sucks ainda minha recebi encomenda\n",
      "Topic 6:  broken fine oz cheaply saver pop made man overpriced rip\n",
      "Topic 7:  terrible ok doesn wash hands age rusty link feel apron\n",
      "Topic 8:  cricut pom trimmer pulls orange rate online neon complex se400\n",
      "Topic 9:  small too ripoff sent pleased back christmas heard gift patern\n",
      "Topic 10:  floss trick teeny brighter ribbon color oz 90 dmc screaming\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "high_ratings = train[train['overall'] == 5]['Review']\n",
    "low_ratings = train[train['overall'] == 1]['Review']\n",
    "\n",
    "high_ratings_sample = high_ratings.sample(n=5000, random_state=42)\n",
    "low_ratings_sample = low_ratings.sample(n=5000, random_state=42)\n",
    "\n",
    "#Vectorizin the text data using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000)\n",
    "tfidf_high = tfidf_vectorizer.fit_transform(high_ratings_sample)\n",
    "tfidf_low = tfidf_vectorizer.fit_transform(low_ratings_sample)\n",
    "\n",
    "#Topic modeling for high ratings\n",
    "lda_high = LatentDirichletAllocation(n_components=10, random_state=42)\n",
    "lda_high.fit(tfidf_high)\n",
    "print(\"Topics for high ratings:\")\n",
    "for idx, topic in enumerate(lda_high.components_):\n",
    "    print(\"Topic {}: \".format(idx + 1), \" \".join([tfidf_vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-11:-1]]))\n",
    "\n",
    "#Topic modeling for low ratings\n",
    "lda_low = LatentDirichletAllocation(n_components=10, random_state=42)\n",
    "lda_low.fit(tfidf_low)\n",
    "print(\"\\nTopics for low ratings:\")\n",
    "for idx, topic in enumerate(lda_low.components_):\n",
    "    print(\"Topic {}: \".format(idx + 1), \" \".join([tfidf_vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b945b7",
   "metadata": {},
   "source": [
    "# Using PyLDA for Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d9cb5048",
   "metadata": {
    "id": "d9cb5048"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\onetw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import gensim.corpora as corpora\n",
    "!pip3 install --upgrade pyLDAvis --user\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56437db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Cleaning: 370863\n",
      "After Cleaning: 310519\n"
     ]
    }
   ],
   "source": [
    "#Cleaning and preprocessing for topic modeling using bertopic\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "train['Review'] = train['Review'].astype(str)\n",
    "train['word_count'] = train['Review'].str.split().apply(len)\n",
    "def filter_reviews(train):\n",
    "    search_strings = [\"nbsp\", \"utf8\", \"cm_cr_arp\", \"ie=\", \"ref\", \"https\", \"txt\", \"ssl\", \"href\", \"dp\"]\n",
    "    pattern = \"|\".join(search_strings)\n",
    "    filtered_indices = train[train['Review'].str.contains(pattern, flags=re.IGNORECASE)].index #review contains any of the above\n",
    "    filtered_indices_word_count = train[train['word_count'] < 3].index\n",
    "    url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+') # removing the hyperlinks\n",
    "    filtered_indices_url = train[train['Review'].str.contains(url_pattern)].index\n",
    "\n",
    "    # Combine all filtered indices\n",
    "    combined_indices = set(filtered_indices) | set(filtered_indices_word_count) | set(filtered_indices_url)\n",
    "\n",
    "    # Drop rows based on filtered indices\n",
    "    filtered_df = train.drop(combined_indices)\n",
    "    filtered_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return filtered_df\n",
    "  # Filter the DataFrame using the defined function\n",
    "filtered_df = filter_reviews(train)\n",
    "# Checking the df size\n",
    "print(\"Before Cleaning:\",len(train))\n",
    "print(\"After Cleaning:\",len(filtered_df))\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = text.split()\n",
    "    cleaned_words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    return ' '.join(cleaned_words)\n",
    "filtered_df['Cleaned_Review'] = filtered_df['Review'].apply(lambda x:clean_text(x))\n",
    "df = filtered_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6551fc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removing reviews with high number of words to remove skewness: 310519\n",
      "After removing : 294655\n"
     ]
    }
   ],
   "source": [
    "#Removing texts that are extremely high with quantile\n",
    "df['text_len'] = [len(text.split()) for text in df.Cleaned_Review]\n",
    "print(\"Before removing reviews with high number of words to remove skewness:\",len(df))\n",
    "df = df[df['text_len'] < df['text_len'].quantile(0.95)]\n",
    "print(\"After removing :\",len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2c8606f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "294655"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "17c28b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing punctaution and tokenizing\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "data = df.Cleaned_Review.values.tolist()\n",
    "data_words = list(sent_to_words(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3f9050c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['works', 'well', 'machine', 'use', 'mostly', 'cones']\n"
     ]
    }
   ],
   "source": [
    "print(data_words[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "45490905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_words)\n",
    "texts = data_words\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "print(corpus[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "UtWra8McWZ2P",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UtWra8McWZ2P",
    "outputId": "d8cd7127-8fb3-4099-a17a-7400f552a220"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.032*\"use\" + 0.017*\"easy\" + 0.016*\"great\" + 0.014*\"yarn\" + 0.012*\"machine\" '\n",
      "  '+ 0.011*\"love\" + 0.011*\"well\" + 0.008*\"work\" + 0.008*\"like\" + '\n",
      "  '0.008*\"needles\"'),\n",
      " (1,\n",
      "  '0.018*\"love\" + 0.017*\"use\" + 0.011*\"perfect\" + 0.011*\"really\" + 0.010*\"get\" '\n",
      "  '+ 0.009*\"nice\" + 0.009*\"paint\" + 0.009*\"good\" + 0.008*\"using\" + '\n",
      "  '0.007*\"brush\"'),\n",
      " (2,\n",
      "  '0.025*\"great\" + 0.022*\"product\" + 0.017*\"used\" + 0.016*\"use\" + 0.015*\"like\" '\n",
      "  '+ 0.013*\"color\" + 0.008*\"really\" + 0.007*\"time\" + 0.007*\"well\" + '\n",
      "  '0.007*\"work\"'),\n",
      " (3,\n",
      "  '0.025*\"good\" + 0.013*\"quality\" + 0.012*\"well\" + 0.011*\"use\" + 0.010*\"love\" '\n",
      "  '+ 0.010*\"colors\" + 0.009*\"like\" + 0.009*\"one\" + 0.009*\"time\" + '\n",
      "  '0.008*\"great\"'),\n",
      " (4,\n",
      "  '0.016*\"good\" + 0.014*\"use\" + 0.011*\"paper\" + 0.011*\"works\" + 0.010*\"love\" + '\n",
      "  '0.009*\"one\" + 0.009*\"much\" + 0.008*\"great\" + 0.007*\"price\" + 0.007*\"using\"'),\n",
      " (5,\n",
      "  '0.026*\"great\" + 0.015*\"love\" + 0.013*\"use\" + 0.012*\"buy\" + 0.011*\"like\" + '\n",
      "  '0.011*\"well\" + 0.010*\"would\" + 0.008*\"much\" + 0.008*\"used\" + 0.008*\"works\"'),\n",
      " (6,\n",
      "  '0.045*\"love\" + 0.028*\"great\" + 0.023*\"nice\" + 0.020*\"good\" + '\n",
      "  '0.014*\"quality\" + 0.013*\"colors\" + 0.013*\"product\" + 0.012*\"well\" + '\n",
      "  '0.011*\"color\" + 0.011*\"work\"'),\n",
      " (7,\n",
      "  '0.016*\"good\" + 0.013*\"one\" + 0.010*\"quality\" + 0.010*\"use\" + 0.008*\"great\" '\n",
      "  '+ 0.008*\"would\" + 0.007*\"price\" + 0.007*\"size\" + 0.007*\"used\" + 0.006*\"im\"'),\n",
      " (8,\n",
      "  '0.061*\"great\" + 0.017*\"price\" + 0.011*\"exactly\" + 0.009*\"needed\" + '\n",
      "  '0.009*\"works\" + 0.009*\"perfect\" + 0.008*\"product\" + 0.008*\"colors\" + '\n",
      "  '0.008*\"quality\" + 0.008*\"used\"'),\n",
      " (9,\n",
      "  '0.017*\"nice\" + 0.016*\"love\" + 0.015*\"would\" + 0.013*\"beads\" + 0.011*\"make\" '\n",
      "  '+ 0.009*\"beautiful\" + 0.008*\"paper\" + 0.008*\"perfect\" + 0.008*\"used\" + '\n",
      "  '0.007*\"little\"')]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "num_topics = 10\n",
    "\n",
    "#LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,id2word=id2word,num_topics=num_topics)\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f43e2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "LDAvis_data_filepath = os.path.join('./results/ldavis_prepared_'+str(num_topics))\n",
    "if 1 == 1:\n",
    "    LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "    with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)\n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "pyLDAvis.save_html(LDAvis_prepared, './results/ldavis_prepared_'+ str(num_topics) +'.html')\n",
    "LDAvis_prepared"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
